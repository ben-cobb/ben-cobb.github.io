@INPROCEEDINGS{10825741,
  author={Cobb, Benjamin and Velasquez, Ricardo and Vuduc, Richard and Park, Haesun},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Clustering and Topic Discovery of Multiway Data via Joint-NCMTF}, 
  year={2024},
  volume={},
  number={},
  pages={1268-1275},
  keywords={Tensors;Big Data;Numerical models;Numerical analysis;approximation methods;text analysis;clustering methods},
  code= {https://gitlab.com/Ben_Cobb/cmtf},
  doi={10.1109/BigData62323.2024.10825741},
  selected={true}
  }

@INPROCEEDINGS{10825324,
  author={Eswar, Srinivas and Hayashi, Koby and Cobb, Benjamin and Kannan, Ramakrishnan and Ballard, Grey and Vuduc, Richard and Park, Haesun},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={On Rank Selection for Nonnegative Matrix Factorization}, 
  year={2024},
  volume={},
  number={},
  pages={1294-1301},
  keywords={Reviews;Big Data;Data models;Matrix decomposition;Rank selection;Nonnegative Matrix Factorization},
  doi={10.1109/BigData62323.2024.10825324}}


@inproceedings{10.1145/3577193.3593733,
author = {Eswar, Srinivas and Cobb, Benjamin and Hayashi, Koby and Kannan, Ramakrishnan and Ballard, Grey and Vuduc, Richard and Park, Haesun},
title = {Distributed-Memory Parallel JointNMF},
year = {2023},
isbn = {9798400700569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577193.3593733},
doi = {10.1145/3577193.3593733},
abstract = {Joint Nonnegative Matrix Factorization (JointNMF) is a hybrid method for mining information from datasets that contain both feature and connection information. We propose distributed-memory parallelizations of three algorithms for solving the JointNMF problem based on Alternating Nonnegative Least Squares, Projected Gradient Descent, and Projected Gauss-Newton. We extend well-known communication-avoiding algorithms using a single processor grid case to our coupled case on two processor grids. We demonstrate the scalability of the algorithms on up to 960 cores (40 nodes) with 60\% parallel efficiency. The more sophisticated Alternating Nonnegative Least Squares (ANLS) and Gauss-Newton variants outperform the first-order gradient descent method in reducing the objective on large-scale problems. We perform a topic modelling task on a large corpus of academic papers that consists of over 37 million paper abstracts and nearly a billion citation relationships, demonstrating the utility and scalability of the methods.},
booktitle = {Proceedings of the 37th ACM International Conference on Supercomputing},
pages = {301â€“312},
numpages = {12},
keywords = {nonnegative matrix factorization, multimodal inputs, high performance computing},
location = {Orlando, FL, USA},
code = {https://github.com/ramkikannan/planc},
video = {https://www.youtube.com/embed/ArJ31ty6clk?si=br8bGxWk6XRagQNj},
series = {ICS '23},
selected={true}
}

@inproceedings{10.1145/3539781.3539798,
author = {Cobb, Benjamin and Kolla, Hemanth and Phipps, Eric and \c{C}ataly\"{u}rek, \"{U}mit V.},
title = {FIST-HOSVD: fused in-place sequentially truncated higher order singular value decomposition},
year = {2022},
isbn = {9781450394109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539781.3539798},
doi = {10.1145/3539781.3539798},
abstract = {In this paper, several novel methods of improving the memory locality of the Sequentially Truncated Higher Order Singular Value Decomposition (ST-HOSVD) algorithm for computing the Tucker decomposition are presented. We show how the two primary computational kernels of the ST-HOSVD can be fused together into a single kernel to significantly improve memory locality. We then extend matrix tiling techniques to tensors to further improve cache utilization. This block-based approach is then coupled with a novel in-place transpose algorithm to drastically reduce the memory requirements of the algorithm by overwriting the original tensor with the result. Our approach's effectiveness is demonstrated by comparing the multi-threaded performance of our optimized ST-HOSVD algorithm to TuckerMPI, a state-of-the-art ST-HOSVD implementation, in compressing two combustion simulation datasets. We demonstrate up to ~ 135x reduction in auxiliary memory consumption thereby increasing the problem size that can be computed for a given memory allocation by up to ~ 3x, whilst maintaining comparable runtime performance.},
booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
articleno = {15},
numpages = {11},
keywords = {tucker decomposition, reduced memory high-water mark, memory efficient, kernel fusion, in-place, data compression, cache blocking},
location = {Basel, Switzerland},
series = {PASC '22},
video = {https://www.youtube.com/embed/6UShsrTOEw0?si=6r-GVuoo4Ep81yXZ},
selected={true}
}